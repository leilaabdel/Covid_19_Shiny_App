{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio-api.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leilaabdel/Covid_19_Shiny_App/blob/master/audio/audio_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPKtwAdgQzil",
        "colab_type": "text"
      },
      "source": [
        "# Audio API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvmKxLO5ba44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_vggish():\n",
        "  pump = vgk.get_pump()\n",
        "  model = vgk.VGGish(pump)\n",
        "\n",
        "  #Create time-stamp for folder name\n",
        "  TIMESTR = time.strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aawWIMDLRkFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: add option for soft vs hard\n",
        "def predict(mp4_filepath, best_model_filepath):\n",
        "    \"\"\"\n",
        "    Outputs:\n",
        "    - A tuple with predictions for each class (positive, neutral, negative)\n",
        "    \"\"\"\n",
        "\n",
        "    model = fer_model()\n",
        "    model.load_model(best_model_filepath)\n",
        "    return model.predict(mp4_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOSIfnb4PRuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class audio_model:\n",
        "    def __init__(self):\n",
        "        self.model = ()\n",
        "        return\n",
        "\n",
        "    def predict(self, mp4_filepath , target_filepath ):\n",
        "        X , y = self.get_feature_batch(mp4_filepath , target_filepath) #Preproccess \n",
        "        X = np.asarray(X) #You may get a bug TODO: trimming \n",
        "\n",
        "        soft_pred = self.model.predict(X)\n",
        "\n",
        "\n",
        "        return soft_pred # (0.1 ,0.2 , 0.7)\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "    def load_model(self, best_model_filepath):\n",
        "        self.model = load_model(best_model_filepath)\n",
        "        \n",
        "        return  \n",
        "\n",
        "    def train(self, X_train , y_train , epochs=50 , batch_size=32 , X_val=None , Y_val=None , val_split=0.1, save_path = None):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Train function with the model architecture\n",
        "\n",
        "        - Outputs\n",
        "          1. Trained model -- saves the model as a .h5 file to the specified path\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        #TODO: There are some bugs here \n",
        "        # Next Steps:\n",
        "        # 1. Transfer learning\n",
        "        # 2. Try CNN-LSTM approach \n",
        "        # 3. For deep networks, try residual blocks\n",
        "        # 4. Look for other model architecture to use with openSmile/VggISH/mel-spec features \n",
        "\n",
        "    \n",
        "        inputs = keras.Input(shape=[32,512])\n",
        "\n",
        "        cnn = \n",
        "\n",
        "        recurrent_1 = keras.layers.Bidirectional(keras.layers.LSTM(10, return_sequences=True, input_shape=[None, 512] ,                                                                   dropout=0.2 , activation='selu')) #A sequence of any length with dimensions 512 (i.e. 512 columns\n",
        "        recurrent_2 = keras.layers.Bidirectional(keras.layers.LSTM(5))\n",
        "        dense_1 = keras.layers.Dense(32 , activation='selu')\n",
        "        dropout_1 = keras.layers.Dropout(0.5)\n",
        "        softmax = keras.layers.Dense(3 , activation='softmax')\n",
        "\n",
        "        x = recurrent_1(inputs)\n",
        "        x = recurrent_2(x)\n",
        "        x = dense_1(x)\n",
        "        x = dropout_1(x)\n",
        "        outputs = softmax(x)\n",
        "\n",
        "        rnn_ae = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=1e-4,\n",
        "        decay_steps=10000,\n",
        "        decay_rate=0.9)\n",
        "        opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        rnn_ae.compile(loss='sparse_categorical_crossentropy' , optimizer=opt , metrics=['accuracy'])\n",
        "\n",
        "        history = None \n",
        "\n",
        "        if X_val == None and Y_val == None:\n",
        "          history = rnn_ae.fit(X_train , y_train , epochs=epochs , batch_size=batch_size,  validation_split=val_split)\n",
        "\n",
        "        else:\n",
        "          history = rnn_ae.fit(X_train , y_train , epochs=epochs , batch_size=batch_size,  validation_data=(X_val, Y_val))\n",
        "\n",
        "\n",
        "        if save_path != None:\n",
        "          assert save_path[-3:] == '.h5'\n",
        "          rnn_ae.save(save_path)\n",
        "        return rnn_ae , history\n",
        "\n",
        "\n",
        "    #Function to slice up the audio.\n",
        "    def slice_audio(self, files, channels, outformat, width, rate, slice_length, slide):\n",
        "        out_files = []\n",
        "        \n",
        "        outformat = outformat.replace('.','').lower()\n",
        "        #Allow the user to see their x-bit selection with this dictionary.\n",
        "        width_translator = {1:'8-bit', 2:'16-bit', 4:'32-bit'}\n",
        "        #For every file in the input list do processing.\n",
        "        for file in files:\n",
        "            print(file)\n",
        "            fileName, fileExtension = os.path.splitext(file)\n",
        "            #Store the file in RAM.\n",
        "            sound = AudioSegment.from_file(file, fileExtension.replace('.','').lower())\n",
        "            #Print the 'x-bit' conversion parameters.\n",
        "            print (width_translator[sound.sample_width]+' to '+width_translator[int(width)]+'.\\n')\n",
        "            #Implement the user-selected or default (if nothing selected) parameters for processing.\n",
        "            sound = sound.set_frame_rate(int(rate))\n",
        "            sound = sound.set_sample_width(int(width))\n",
        "            sound = sound.set_channels(int(channels))\n",
        "            length_sound_ms = len(sound)\n",
        "            length_slice_ms = int(slice_length)\n",
        "            slice_start = 0\n",
        "            #create audiosegment object\n",
        "            notes_reversed = sound[0:1].reverse()\n",
        "            #Begin slicing at the start of the file.\n",
        "            while slice_start + length_slice_ms < length_sound_ms:\n",
        "                sound_slice = sound[slice_start:slice_start+length_slice_ms]\n",
        "                backwards = sound_slice.reverse()\n",
        "                notes_reversed += backwards\n",
        "                sound_slice.export('to_zip/'+ fileName+'.slice'+str(slice_start/1000)+'SecsTo'+str((slice_start+length_slice_ms)/1000)+'Secs.'+outformat, format=outformat)\n",
        "                \n",
        "                out_files.append('to_zip/'+ fileName+'.slice'+str(slice_start/1000)+'SecsTo'+str((slice_start+length_slice_ms)/1000)+'Secs.'+outformat)\n",
        "                \n",
        "                #backwards.export( fileName+'backwards_slice'+str(slice_start/1000)+'SecsTo'+str((slice_start+length_slice_ms)/1000)+'Secs.'+outformat, format=outformat)\n",
        "                slice_start += int(slide)\n",
        "            #When the slice is abutting the end of the file, output that slice too.'\n",
        "            if slice_start + length_slice_ms >= length_sound_ms:\n",
        "                sound_slice = sound[slice_start:length_sound_ms]\n",
        "                backwards = sound_slice.reverse()\n",
        "                notes_reversed += backwards\n",
        "                sound_slice.export('to_zip/'+fileName+'.slice'+str(slice_start/1000)+'SecsToEndFileAt'+str((length_sound_ms)/1000)+'Secs.'+outformat, format=outformat)\n",
        "                \n",
        "                out_files.append('to_zip/'+fileName+'.slice'+str(slice_start/1000)+'SecsToEndFileAt'+str((length_sound_ms)/1000)+'Secs.'+outformat)\n",
        "                \n",
        "                #backwards.export(fileName+'backwards_slice'+str(slice_start/1000)+'SecsToEndFileAt'+str((slice_start+length_slice_ms)/1000)+'Secs.'+outformat, format=outformat)\n",
        "            #Save the sewn together backwards bits to file\n",
        "            #notes_reversed.export(fileName+'notes_reversed_granular.'+outformat, format=outformat)\n",
        "\n",
        "        return out_files\n",
        "    def preprocess(self, mp4_filepath , target_label_path=None , hop_int=1):\n",
        "\n",
        "      \"\"\"\n",
        "      Outputs:\n",
        "      - A numpy array with dimensions (m,n). \n",
        "        - m is the units in time dependent on the audio splice rate.\n",
        "        - n is the number of features from the openSMILE library.\n",
        "      \"\"\"\n",
        "\n",
        "      print(f\"Processing file {mp4_filepath} ...\")\n",
        "\n",
        "\n",
        "      output_wav_file = mp4_filepath.split(\"/\")[-1] + 'extracted_audio.wav'\n",
        "      mp4_filename = os.path.basename(mp4_filepath)\n",
        "      audio_home_dir = os.path.dirname(mp4_filepath)\n",
        "\n",
        "      # Strip the audio from video and store as .wav file\n",
        "      ffmpeg_extract_audio(mp4_filepath, output_wav_file)\n",
        "\n",
        "      files_written = self.slice_audio([output_wav_file], 2, \"wav\", 2, 30000, 2000, 100)\n",
        "\n",
        "      X_arr = []\n",
        "\n",
        "      # VGGish feature extraction\n",
        "      #out_fn = os.path.join('/content/openSmile-features.arff')\n",
        "      counter = 0\n",
        "      for in_fn in files_written:\n",
        "        name = os.path.basename(in_fn)\n",
        "        if counter % hop_int == 0: # Choose every hop_int splice\n",
        "          X = pump.transform(in_fn)[vgk.params.PUMP_INPUT]\n",
        "          X_arr.append(X)        \n",
        "        counter += 1\n",
        "\n",
        "      for in_fn in files_written:\n",
        "        os.remove(in_fn)\n",
        "\n",
        "      # Get the Y values \n",
        "      target = None \n",
        "      if target_label_path is not None:\n",
        "          target_labels = np.genfromtxt(target_label_path , delimiter = ' ' , dtype='str')\n",
        "          target_index = np.where(target_labels[: , 0] == mp4_filename[:-4])[0]\n",
        "          target = int(target_labels[: , 1][target_index])\n",
        "\n",
        "      \n",
        "      return X_arr , target\n",
        "\n",
        "\n",
        "      # Read in each video file and add the (m,n) feature matrix to a 3D array\n",
        "\n",
        "    def get_feature_batch(self, input_files_dir , batch_size=3000 , target_label_path=None):\n",
        "        \"\"\"\n",
        "        Inputs: \n",
        "        - Path to the .mp4 files\n",
        "        Outputs:\n",
        "        - An ndarray with dims (s , m , n)\n",
        "          - s is the number of samples\n",
        "          - m is the number of slices for that sample (32)\n",
        "          - n is the number of features (512)\n",
        "        \"\"\"\n",
        "\n",
        "        output_x = []\n",
        "        output_y = None\n",
        "\n",
        "        if target_label_path is not None:\n",
        "          output_y = []\n",
        "\n",
        "\n",
        "        counter = 1\n",
        "\n",
        "        fileList = glob.glob(input_files_dir + '*.mp4')\n",
        "\n",
        "        MAX_WORKERS = 32\n",
        "\n",
        "        futures = []\n",
        "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            for file_path in fileList:\n",
        "            \n",
        "                print(f\"Submitting {counter} - {file_path}\")\n",
        "\n",
        "                # one_sample_feat_matrix , y = self.preprocess(file_path , target_label_path=target_label_path)\n",
        "\n",
        "                future = executor.submit(self.preprocess, file_path, target_label_path)\n",
        "                futures.append(future)\n",
        "\n",
        "                if counter >= batch_size:\n",
        "                    break\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "        print(\"***** Submitted all tasks *****\")\n",
        "\n",
        "        k = 0\n",
        "        for future in futures:\n",
        "            X_arr, y = future.result()\n",
        "\n",
        "            Z_arr = []\n",
        "            for X in X_arr:\n",
        "                # np.concatenate([X]*5)\n",
        "                Z = model.predict(X)\n",
        "                Z_arr += [Z]\n",
        "\n",
        "            Z_arr = np.asarray(Z_arr , dtype='float32')\n",
        "            Z_arr = Z_arr.squeeze()\n",
        "\n",
        "            # Standardize\n",
        "            scaler = StandardScaler()\n",
        "            last_index = min(Z_arr.shape[0], 32)\n",
        "            Z_arr = Z_arr[:last_index] #Trim \n",
        "            all_timepoints_feature_array = scaler.fit_transform(Z_arr)\n",
        "\n",
        "            print(all_timepoints_feature_array.shape)\n",
        "\n",
        "            print(f\"Finished future {k}\")\n",
        "            output_x.append(all_timepoints_feature_array)\n",
        "            if target_label_path is not None:        \n",
        "              output_y.append(y)\n",
        "            future.result()\n",
        "            k += 1\n",
        "        print(\"***** Completed *****\")\n",
        "\n",
        "        output_y = np.asarray(output_y)\n",
        "        output_y -= 1\n",
        "        output_x = np.asarray(output_x)\n",
        "        output_np_x = np.zeros((len(output_x) , 32 , 512))\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(len(output_x)):\n",
        "          for j in range(output_x[i].shape[0]):\n",
        "            for k in range(512):\n",
        "              output_np_x[i][j][k] = output_x[i][j][k]\n",
        "\n",
        "       \n",
        "\n",
        "        return output_np_x , output_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5lfxK8Nb5A8",
        "colab_type": "text"
      },
      "source": [
        "# Main Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBAbNurmQFAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if __name__ == \"__main__\":\n",
        "\n",
        "import subprocess\n",
        "\n",
        "subprocess.check_output(\"pip install pydub\", shell=True)\n",
        "subprocess.check_output(\"pip install vggish-keras==0.0.18\", shell=True)\n",
        "subprocess.check_output(\"pip install git+https://github.com/beasteers/pumpp@tf_keras\", shell=True) #Update the path base.py file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' , force_remount=True)\n",
        "#Patch\n",
        "subprocess.check_output(\"cp -f '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n-project/notebooks/base.py' '/usr/local/lib/python3.6/dist-packages/pumpp/feature/base.py'\", shell=True)\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_audio\n",
        "import glob\n",
        "import time\n",
        "import importlib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "import librosa\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "from pydub import AudioSegment\n",
        "import argparse\n",
        "import audioread\n",
        "import time\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "import vggish_keras as vgk\n",
        "importlib.reload(vgk)\n",
        "pump = vgk.get_pump()\n",
        "model = vgk.VGGish(pump) \n",
        "\n",
        "# Instantiate audio_model object\n",
        "model_test = audio_model()\n",
        "\n",
        "# Extract and preprocess X_train and Y_train from .mp4 file paths\n",
        "\n",
        "subprocess.check_output(\"cp '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n-project/datasets/emotiw/Train.zip' '/content/'\", shell=True)\n",
        "subprocess.check_output(\"unzip Train.zip\", shell=True)\n",
        "subprocess.check_output(\"mkdir to_zip\", shell = True)\n",
        "\n",
        "subprocess.check_output(\"cp '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n-project/datasets/emotiw/Train_labels.txt' '/content/'\", shell=True)\n",
        "X_train, Y_train = model_test.get_feature_batch('Train/' , batch_size=3000 , target_label_path='Train_labels.txt')\n",
        "with open('/content/gdrive/My Drive/Machine-Learning-Projects/cs231n-project/notebooks/audio-new/arrays/np_arrays/100ms-32-unit-seq-vggish.x_train.pickle', 'wb') as f:\n",
        "  pickle.dump(X_train, f) \n",
        "with open('/content/gdrive/My Drive/Machine-Learning-Projects/cs231n-project/notebooks/audio-new/arrays/np_arrays/100ms-32-unit-seq-vggish.y_train.pickle', 'wb') as f:\n",
        "  pickle.dump(Y_train, f)   \n",
        "# Train the model\n",
        "\n",
        "model_test = audio_model()\n",
        "model, history = model_test.train(X_train_new, Y_train , save_path='/content/gdrive/My Drive/Machine-Learning-Projects/cs231n-project/notebooks/audio-new/models/api_train_test-1.h5' )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtgHGMFzJqcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f12f4f05-2c54-47bb-c872-81fb2e8c9808"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/50\n",
            "75/75 [==============================] - 11s 147ms/step - loss: 1.1802 - accuracy: 0.3446 - val_loss: 1.1486 - val_accuracy: 0.3184\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - 10s 131ms/step - loss: 1.1764 - accuracy: 0.3463 - val_loss: 1.1386 - val_accuracy: 0.3446\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.1448 - accuracy: 0.3534 - val_loss: 1.1318 - val_accuracy: 0.3258\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 1.1447 - accuracy: 0.3517 - val_loss: 1.1274 - val_accuracy: 0.3071\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.1386 - accuracy: 0.3392 - val_loss: 1.1275 - val_accuracy: 0.3296\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.1284 - accuracy: 0.3555 - val_loss: 1.1208 - val_accuracy: 0.3258\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.1171 - accuracy: 0.3759 - val_loss: 1.1165 - val_accuracy: 0.3296\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.1165 - accuracy: 0.3555 - val_loss: 1.1135 - val_accuracy: 0.3109\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 1.1078 - accuracy: 0.3764 - val_loss: 1.1120 - val_accuracy: 0.3333\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.1108 - accuracy: 0.3801 - val_loss: 1.1149 - val_accuracy: 0.3333\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.1010 - accuracy: 0.3797 - val_loss: 1.1099 - val_accuracy: 0.3521\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.1035 - accuracy: 0.3747 - val_loss: 1.1103 - val_accuracy: 0.3371\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0961 - accuracy: 0.3968 - val_loss: 1.1069 - val_accuracy: 0.3521\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 1.0935 - accuracy: 0.3968 - val_loss: 1.1062 - val_accuracy: 0.3258\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 1.0968 - accuracy: 0.3901 - val_loss: 1.1053 - val_accuracy: 0.3483\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0847 - accuracy: 0.4002 - val_loss: 1.1083 - val_accuracy: 0.3333\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0866 - accuracy: 0.3985 - val_loss: 1.1060 - val_accuracy: 0.3408\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0785 - accuracy: 0.4173 - val_loss: 1.1083 - val_accuracy: 0.3408\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0809 - accuracy: 0.4227 - val_loss: 1.1078 - val_accuracy: 0.3446\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0645 - accuracy: 0.4244 - val_loss: 1.1012 - val_accuracy: 0.3596\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0739 - accuracy: 0.4244 - val_loss: 1.1034 - val_accuracy: 0.3408\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 1.0639 - accuracy: 0.4273 - val_loss: 1.0983 - val_accuracy: 0.3521\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0492 - accuracy: 0.4549 - val_loss: 1.0970 - val_accuracy: 0.3633\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 1.0468 - accuracy: 0.4507 - val_loss: 1.0983 - val_accuracy: 0.3521\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0528 - accuracy: 0.4419 - val_loss: 1.0976 - val_accuracy: 0.3633\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - 10s 131ms/step - loss: 1.0510 - accuracy: 0.4332 - val_loss: 1.0942 - val_accuracy: 0.3820\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 1.0503 - accuracy: 0.4490 - val_loss: 1.1004 - val_accuracy: 0.3745\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0313 - accuracy: 0.4795 - val_loss: 1.0950 - val_accuracy: 0.3670\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0225 - accuracy: 0.4904 - val_loss: 1.0895 - val_accuracy: 0.3858\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 1.0238 - accuracy: 0.4799 - val_loss: 1.0879 - val_accuracy: 0.3895\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0116 - accuracy: 0.4954 - val_loss: 1.0911 - val_accuracy: 0.3933\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0165 - accuracy: 0.4954 - val_loss: 1.0852 - val_accuracy: 0.3708\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 1.0049 - accuracy: 0.4954 - val_loss: 1.0952 - val_accuracy: 0.4007\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 0.9960 - accuracy: 0.5100 - val_loss: 1.0976 - val_accuracy: 0.3633\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 0.9910 - accuracy: 0.5084 - val_loss: 1.0937 - val_accuracy: 0.3820\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 0.9851 - accuracy: 0.5188 - val_loss: 1.0945 - val_accuracy: 0.3858\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 0.9804 - accuracy: 0.5138 - val_loss: 1.0884 - val_accuracy: 0.3970\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 0.9622 - accuracy: 0.5301 - val_loss: 1.0814 - val_accuracy: 0.4232\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 0.9619 - accuracy: 0.5242 - val_loss: 1.0971 - val_accuracy: 0.3783\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 0.9494 - accuracy: 0.5518 - val_loss: 1.0840 - val_accuracy: 0.4157\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 0.9493 - accuracy: 0.5485 - val_loss: 1.0859 - val_accuracy: 0.4082\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - 10s 131ms/step - loss: 0.9422 - accuracy: 0.5660 - val_loss: 1.0855 - val_accuracy: 0.4157\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - 10s 132ms/step - loss: 0.9267 - accuracy: 0.5781 - val_loss: 1.0852 - val_accuracy: 0.4082\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - 10s 133ms/step - loss: 0.9258 - accuracy: 0.5827 - val_loss: 1.0869 - val_accuracy: 0.4157\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - 10s 138ms/step - loss: 0.9118 - accuracy: 0.5652 - val_loss: 1.0912 - val_accuracy: 0.4195\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - 10s 134ms/step - loss: 0.9139 - accuracy: 0.5802 - val_loss: 1.0844 - val_accuracy: 0.4120\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 0.9143 - accuracy: 0.5794 - val_loss: 1.0960 - val_accuracy: 0.3895\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 0.8863 - accuracy: 0.5877 - val_loss: 1.1017 - val_accuracy: 0.4045\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 0.8834 - accuracy: 0.5973 - val_loss: 1.0949 - val_accuracy: 0.4045\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 0.8799 - accuracy: 0.5965 - val_loss: 1.0954 - val_accuracy: 0.4419\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}